# AI-Powered SOC Log Intelligence System

A production-grade Security Operations Center (SOC) log intelligence platform that ingests security logs, detects anomalies using machine learning, correlates events into incidents, and provides explainable alerts mapped to MITRE ATT&CK techniques.

## ğŸ¯ Features

- **Multi-Source Log Ingestion**: Supports authentication, network, and process/command logs
- **ML-Based Anomaly Detection**: Uses Isolation Forest, LSTM Autoencoder, and Dense Autoencoder
- **Intelligent Incident Correlation**: Groups alerts by entity (user/IP) and time window
- **MITRE ATT&CK Mapping**: Automatically maps detections to adversary tactics and techniques
- **Explainable AI**: Provides feature attribution with percentiles and z-scores
- **SOC Workflow Support**: Incident status tracking, analyst notes, and investigation tools
- **Real-Time Dashboard**: Streamlit-based visualization with interactive investigation views

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Raw Log Sources                          â”‚
â”‚  (Auth, Network, Process/Command Logs - Zeek, Suricata, etc.)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Ingestion Layer                       â”‚
â”‚                    POST /ingest/logs                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Log Processing & Normalization                       â”‚
â”‚  â€¢ Parse raw logs â†’ structured JSON                              â”‚
â”‚  â€¢ Normalize timestamps, users, IPs                              â”‚
â”‚  â€¢ Sessionize events by user/IP (gap-based)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Feature Engineering                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Window Features  â”‚         â”‚ Sequence Featuresâ”‚            â”‚
â”‚  â”‚ â€¢ Event frequencyâ”‚         â”‚ â€¢ Token sequences â”‚            â”‚
â”‚  â”‚ â€¢ Rare commands  â”‚         â”‚ â€¢ Order patterns  â”‚            â”‚
â”‚  â”‚ â€¢ Login deviationâ”‚         â”‚ â€¢ Sequence length â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                            â”‚
            â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Isolation Forest     â”‚    â”‚  LSTM Autoencoder    â”‚
â”‚  (Point Anomalies)    â”‚    â”‚  (Sequence Anomalies)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                            â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Dense Autoencoder   â”‚
            â”‚  (Reconstruction)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Detection & Classification  â”‚
        â”‚  â€¢ Score fusion (window+seq)  â”‚
        â”‚  â€¢ Detection type mapping     â”‚
        â”‚  â€¢ MITRE ATT&CK assignment    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Incident Correlation       â”‚
        â”‚  â€¢ Group by entity+time      â”‚
        â”‚  â€¢ Severity assignment       â”‚
        â”‚  â€¢ Risk score normalization  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Alert Storage & API         â”‚
        â”‚  â€¢ SQLite database           â”‚
        â”‚  â€¢ REST endpoints            â”‚
        â”‚  â€¢ Incident lifecycle        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Streamlit Dashboard        â”‚
        â”‚  â€¢ Alert timeline            â”‚
        â”‚  â€¢ Investigation views       â”‚
        â”‚  â€¢ Analytics & metrics       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Machine Learning Models

### 1. Isolation Forest
- **Purpose**: Detects rare point anomalies in window-based features
- **Input**: 13-dimensional feature vectors (auth failures, port counts, bytes, etc.)
- **Output**: Anomaly score [0, 1] indicating how isolated a window is

### 2. LSTM Autoencoder
- **Purpose**: Detects abnormal behavior sequences
- **Input**: Tokenized event sequences (max length: 50)
- **Architecture**: Encoder (LSTM) â†’ Latent â†’ Decoder (LSTM)
- **Output**: Reconstruction error as anomaly score

### 3. Dense Autoencoder
- **Purpose**: Detects reconstruction-based anomalies in window features
- **Input**: Same 13-dimensional window features as Isolation Forest
- **Architecture**: 13 â†’ 8 â†’ 4 â†’ 8 â†’ 13 (bottleneck)
- **Output**: Reconstruction error as anomaly score

### Score Fusion
Final risk score combines all three models:
```
final_risk = 0.6 * window_score + 0.4 * sequence_score
```
Where `window_score` blends Isolation Forest and Dense AE scores.

## ğŸ“Š Detection Pipeline

### 1. Log Ingestion
- Accepts NDJSON format logs via `POST /ingest/logs`
- Supports multiple sources: `auth`, `network`, `process`
- Normalizes timestamps, extracts entities (user, IP, host)

### 2. Feature Extraction

**Window Features** (5-minute windows):
- `auth_failures`: Count of failed login attempts
- `auth_successes`: Count of successful logins
- `distinct_users_in_window`: Unique users in window
- `distinct_dst_ips`: Unique destination IPs
- `distinct_dst_ports`: Unique destination ports
- `deny_rate`: Network deny rate
- `bytes_sum`: Total bytes transferred
- `process_exec_count`: Process execution count
- `rare_command_count`: Commands seen â‰¤3 times globally
- `max_rare_command_rarity`: Maximum rarity score
- `login_hour_deviation_z`: Z-score of login hour vs. user profile
- `event_sequence_len`: Length of event sequence
- `distinct_sources`: Number of distinct log sources

**Sequence Features**:
- Tokenized event sequences (one-hot encoded)
- Sequence length and ordering patterns

### 3. Anomaly Detection
- Models score each window/sequence
- Signals generated if score > threshold (default: 0.25)
- Signals include evidence (raw events) and feature vectors

### 4. Detection Classification
Maps anomalies to specific security detections:
- `brute_force_login`: Multiple failed logins from same IP
- `credential_stuffing`: 15+ authentication failures
- `port_scan`: Many distinct ports, low bytes
- `lateral_movement_attempt`: Multiple IPs with admin ports
- `exfil_spike`: Large data transfer spike
- `abnormal_service_usage`: Service account with unusual activity
- `anomalous_behavior`: Generic fallback

### 5. Incident Correlation
- Groups signals by entity (user OR IP) within time window (default: 15 min)
- Creates incidents when â‰¥2 signals correlate
- Assigns severity using quantile-based distribution
- Applies logistic scaling to avoid score saturation (range: 0.4-0.95)

### 6. MITRE ATT&CK Mapping
Automatically maps detection types to MITRE techniques:
- `bruteforce` â†’ T1110 (Brute Force)
- `valid_accounts` â†’ T1078 (Valid Accounts)
- `port_scan` â†’ T1046 (Network Service Scanning)
- `remote_services` â†’ T1021 (Remote Services)
- `ingress_tool_transfer` â†’ T1105 (Ingress Tool Transfer)

##  Quick Start

### Prerequisites
- Python 3.10+
- Virtual environment (recommended)

### Installation

1. **Clone and setup**:
```bash
cd "SOC SYSTEM"
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# or: source .venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

2. **Generate sample logs**:
```bash
python -m soclsim.generate --out data/raw --days 7 --seed 7
```

3. **Train models**:
```bash
python -m soclsim.train --raw data/raw --artifacts artifacts --epochs 8
```

4. **Start API server** (Terminal 1):
```bash
uvicorn soclsim.api.main:app --host 127.0.0.1 --port 8000
```

5. **Ingest logs and start dashboard** (Terminal 2):
```bash
python -m soclsim.ingest_file --api http://127.0.0.1:8000 --raw data/raw --batch 500
streamlit run soclsim/dashboard/app.py
```

6. **Access dashboard**: Open `http://localhost:8501` in your browser

## ğŸ“¡ API Endpoints

### `POST /ingest/logs`
Ingest raw security logs.

**Request**:
```json
{
  "source": "auth",
  "events": [
    {"ts": "2026-01-15T08:00:00Z", "user": "alice", "ip": "10.0.1.5", ...}
  ]
}
```

**Response**:
```json
{
  "ingested": 500,
  "new_alerts": 3
}
```

### `GET /alerts`
Retrieve alerts with filtering.

**Query Parameters**:
- `min_severity`: `low`, `medium`, or `high` (default: `low`)
- `limit`: Max results (default: 100)
- `user`: Filter by username
- `ip`: Filter by IP address

### `GET /incidents`
List all incidents.

**Query Parameters**:
- `limit`: Max results (default: 100)

### `GET /incidents/{incident_id}`
Get incident details with associated alerts.

### `PATCH /incidents/{incident_id}`
Update incident status or analyst notes.

**Query Parameters**:
- `status`: `open`, `investigating`, or `resolved`
- `analyst_notes`: Free-text notes

### `GET /stats`
Get system statistics and top risk entities.

## ğŸ¨ Dashboard Features

### Alerts Timeline
- Interactive scatter plot showing alerts over time
- Color-coded by severity
- Hover for details

### Incident Management
- List all incidents with summary information
- Select incident to view details
- Update status and add analyst notes
- View all alerts within an incident

### Alert Investigation
Collapsible sections:
- **Risk Score Breakdown**: Final risk, window score, sequence score, model agreement
- **Detection Information**: Severity, detection type, category
- **MITRE ATT&CK Mapping**: Mapped techniques with tactic icons
- **Top Contributing Features**: Feature attribution with percentiles
- **Evidence Timeline**: All correlated log events (expandable)
- **Related Alerts**: Previous alerts for same IP/user
- **Incident Context**: Timeline of alerts within incident

### Analytics
- **Severity Distribution**: Bar chart of alert severities
- **Detection Category Distribution**: Bar chart of detection types
- **Alerts per Hour**: Histogram showing alert frequency over time

### Top Risk Entities
- **Users**: Weighted risk score (max score Ã— 0.4 + incidents Ã— 0.3 + severity Ã— 0.2 + recency Ã— 0.1)
- **IPs**: Same weighted calculation
- Shows alert count and incident count

## âš™ï¸ Configuration

Environment variables (with defaults):

```bash
# Time windows
SOCLSIM_SESSION_GAP_MINUTES=30      # Session inactivity gap
SOCLSIM_WINDOW_MINUTES=5            # Feature window size
SOCLSIM_CORRELATION_WINDOW_MINUTES=15  # Incident grouping window

# Detection thresholds
SOCLSIM_SIGNAL_MIN_SCORE=0.25       # Minimum score to create signal
SOCLSIM_INCIDENT_MIN_SIGNALS=2      # Signals needed for incident

# Severity thresholds
SOCLSIM_SEVERITY_MEDIUM=0.5         # Medium severity threshold
SOCLSIM_SEVERITY_HIGH=0.8           # High severity threshold

# Sequence length
SOCLSIM_MAX_SEQUENCE_LEN=50         # Max sequence tokens
```

## ğŸ“ Project Structure

```
soclsim/
â”œâ”€â”€ api/              # FastAPI application
â”‚   â””â”€â”€ main.py       # REST endpoints
â”œâ”€â”€ correlation/      # Incident correlation engine
â”‚   â”œâ”€â”€ correlate.py  # Signal grouping logic
â”‚   â””â”€â”€ mitre.py      # MITRE ATT&CK mappings
â”œâ”€â”€ dashboard/        # Streamlit dashboard
â”‚   â””â”€â”€ app.py        # UI components
â”œâ”€â”€ detection/        # Detection classification
â”‚   â””â”€â”€ classify.py   # Detection type mapping
â”œâ”€â”€ features/         # Feature engineering
â”‚   â”œâ”€â”€ windows.py    # Window-based features
â”‚   â””â”€â”€ sequences.py  # Sequence features
â”œâ”€â”€ logs/             # Log processing
â”‚   â”œâ”€â”€ parsers.py    # Log parsing
â”‚   â”œâ”€â”€ sessionize.py # Sessionization
â”‚   â””â”€â”€ generate.py   # Sample log generator
â”œâ”€â”€ models/           # ML models
â”‚   â”œâ”€â”€ isoforest.py  # Isolation Forest
â”‚   â”œâ”€â”€ torch_models.py # PyTorch models
â”‚   â”œâ”€â”€ train_torch.py # Training scripts
â”‚   â””â”€â”€ scoring.py    # Inference & explainability
â”œâ”€â”€ runtime/          # Runtime components
â”‚   â”œâ”€â”€ artifacts.py  # Model artifact loading
â”‚   â”œâ”€â”€ engine.py     # Detection engine
â”‚   â””â”€â”€ state.py      # In-memory state
â”œâ”€â”€ db.py             # Database models & operations
â”œâ”€â”€ schemas.py        # Pydantic schemas
â”œâ”€â”€ config.py         # Configuration
â””â”€â”€ train.py          # Main training script
```

## ğŸ”§ Development

### Running Tests
```bash
python -m pytest tests/
```

### Code Quality
```bash
black soclsim/
flake8 soclsim/
mypy soclsim/
```

### Adding New Log Sources
1. Add parser in `soclsim/logs/parsers.py`
2. Update `parse_any()` function
3. Regenerate training data
4. Retrain models

## ğŸ“ˆ Performance

- **Ingestion**: ~1000 events/second
- **Detection Latency**: <100ms per batch
- **Dashboard Load Time**: <2s for 10k alerts

## ğŸ”’ Security Considerations

- All timestamps normalized to UTC
- SQL injection protection via SQLModel
- Input validation via Pydantic
- Rate limiting recommended for production

## ğŸ“ License

MIT License

